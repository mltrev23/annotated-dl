{
 "<h1>Low-Rank Adaptation (LoRA)</h1>\n<p>This is an implementation of <a href=\"https://arxiv.org/abs/2106.09685\">Low-Rank Adaptation (LoRA)</a> in <a href=\"https://pytorch.org\">PyTorch</a>.</p>\n<p>Low-Rank Adaptation (LoRA) freezes pre-trained model weights and injects  trainable rank decomposition matrices into each layer of the transformer.  This makes it possible to efficiently fine-tune large langauge models by  reducing trainable parameters by a large factor.</p>\n<p>Here&#x27;s <a href=\"experiment.html\">the training code</a> for training a GPT2 model with LoRA  on Tiny Shakespeare dataset.</p>\n": "<h1>Low-Rank Adaptation (LoRA)</h1>\n<p>This is an implementation of <a href=\"https://arxiv.org/abs/2106.09685\">Low-Rank Adaptation (LoRA)</a> in <a href=\"https://pytorch.org\">PyTorch</a>.</p>\n<p>Low-Rank Adaptation (LoRA) freezes pre-trained model weights and injects  trainable rank decomposition matrices into each layer of the transformer.  This makes it possible to efficiently fine-tune large langauge models by  reducing trainable parameters by a large factor.</p>\n<p>Here&#x27;s <a href=\"experiment.html\">the training code</a> for training a GPT2 model with LoRA  on Tiny Shakespeare dataset.</p>\n",
 "<h2>LoRA Embedding Layer</h2>\n<p>Similar to LoRA linear layer this adds a low-rank decomposition to the pre-trained embedding weights matrix (<span translate=no>_^_0_^_</span>).</p>\n<p><span translate=no>_^_1_^_</span></p>\n": "<h2>LoRA Embedding Layer</h2>\n<p>Similar to LoRA linear layer this adds a low-rank decomposition to the pre-trained embedding weights matrix (<span translate=no>_^_0_^_</span>).</p>\n<p><span translate=no>_^_1_^_</span></p>\n",
 "<h2>LoRA Linear Layer</h2>\n<p>LoRA linear layer adds a low-rank decomposition to the pre-trained weight matrix (<span translate=no>_^_0_^_</span>) of the linear layer.</p>\n<p><span translate=no>_^_1_^_</span></p>\n<p>, where <span translate=no>_^_2_^_</span>, <span translate=no>_^_3_^_</span>,  and the rank <span translate=no>_^_4_^_</span>.</p>\n<p>All parameters are frozen except <span translate=no>_^_5_^_</span> and <span translate=no>_^_6_^_</span>.</p>\n<p><span translate=no>_^_7_^_</span> is initialized to be zero at the beginning of the training.</p>\n<p>They multiple <span translate=no>_^_8_^_</span> by <span translate=no>_^_9_^_</span> where <span translate=no>_^_10_^_</span> is a hyper-parameter. Once <span translate=no>_^_11_^_</span> is tuned it can be kept the same when varying <span translate=no>_^_12_^_</span>.</p>\n": "<h2>LoRA Linear Layer</h2>\n<p>LoRA linear layer adds a low-rank decomposition to the pre-trained weight matrix (<span translate=no>_^_0_^_</span>) of the linear layer.</p>\n<p><span translate=no>_^_1_^_</span></p>\n<p>, where <span translate=no>_^_2_^_</span>, <span translate=no>_^_3_^_</span>,  and the rank <span translate=no>_^_4_^_</span>.</p>\n<p>All parameters are frozen except <span translate=no>_^_5_^_</span> and <span translate=no>_^_6_^_</span>.</p>\n<p><span translate=no>_^_7_^_</span> is initialized to be zero at the beginning of the training.</p>\n<p>They multiple <span translate=no>_^_8_^_</span> by <span translate=no>_^_9_^_</span> where <span translate=no>_^_10_^_</span> is a hyper-parameter. Once <span translate=no>_^_11_^_</span> is tuned it can be kept the same when varying <span translate=no>_^_12_^_</span>.</p>\n",
 "<p> </p>\n": "<p> </p>\n",
 "<p>Add <span translate=no>_^_0_^_</span> </p>\n": "<p>Add <span translate=no>_^_0_^_</span> </p>\n",
 "<p>Bias parameter <span translate=no>_^_0_^_</span> (also frozen) </p>\n": "<p>Bias parameter <span translate=no>_^_0_^_</span> (also frozen) </p>\n",
 "<p>Compute <span translate=no>_^_0_^_</span> </p>\n": "<p>Compute <span translate=no>_^_0_^_</span> </p>\n",
 "<p>Compute the embeddings <span translate=no>_^_0_^_</span> </p>\n": "<p>Compute the embeddings <span translate=no>_^_0_^_</span> </p>\n",
 "<p>Freeze it </p>\n": "<p>Freeze it </p>\n",
 "<p>Initialize <span translate=no>_^_0_^_</span> similar to a weight matrix in a normal linear layer </p>\n": "<p>Initialize <span translate=no>_^_0_^_</span> similar to a weight matrix in a normal linear layer </p>\n",
 "<p>Initialize <span translate=no>_^_0_^_</span> to <span translate=no>_^_1_^_</span> so that <span translate=no>_^_2_^_</span> is <span translate=no>_^_3_^_</span> at initialization </p>\n": "<p>Initialize <span translate=no>_^_0_^_</span> to <span translate=no>_^_1_^_</span> so that <span translate=no>_^_2_^_</span> is <span translate=no>_^_3_^_</span> at initialization </p>\n",
 "<p>Initialize <span translate=no>_^_0_^_</span> with a normal distribution </p>\n": "<p>Initialize <span translate=no>_^_0_^_</span> with a normal distribution </p>\n",
 "<p>Matrix <span translate=no>_^_0_^_</span> </p>\n": "<p>Matrix <span translate=no>_^_0_^_</span> </p>\n",
 "<p>Matrix <span translate=no>_^_0_^_</span>, we keep <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> transposed </p>\n": "<p>Matrix <span translate=no>_^_0_^_</span>, we keep <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> transposed </p>\n",
 "<p>No bias parameter </p>\n": "<p>No bias parameter </p>\n",
 "<p>Set <span translate=no>_^_0_^_</span> is not provided. i.e. make the scaling factor <span translate=no>_^_1_^_</span>. </p>\n": "<p>Set <span translate=no>_^_0_^_</span> is not provided. i.e. make the scaling factor <span translate=no>_^_1_^_</span>. </p>\n",
 "<p>The pre-trained embedding weights <span translate=no>_^_0_^_</span> (frozen) </p>\n": "<p>The pre-trained embedding weights <span translate=no>_^_0_^_</span> (frozen) </p>\n",
 "<p>The pre-trained weight <span translate=no>_^_0_^_</span> </p>\n": "<p>The pre-trained weight <span translate=no>_^_0_^_</span> </p>\n",
 "<p>scaling factor <span translate=no>_^_0_^_</span> </p>\n": "<p>scaling factor <span translate=no>_^_0_^_</span> </p>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  is the number of embeddings </li>\n<li><span translate=no>_^_1_^_</span>  is the number embedding dimensions </li>\n<li><span translate=no>_^_2_^_</span>  is the rank of the decomposition <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span>  is the scaling factor <span translate=no>_^_5_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>  is the number of embeddings </li>\n<li><span translate=no>_^_1_^_</span>  is the number embedding dimensions </li>\n<li><span translate=no>_^_2_^_</span>  is the rank of the decomposition <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span>  is the scaling factor <span translate=no>_^_5_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  is the number of input features of the linear layer </li>\n<li><span translate=no>_^_1_^_</span>  is the number of output features of the linear layer </li>\n<li><span translate=no>_^_2_^_</span>  is a flag indicating if there is a bias parameter </li>\n<li><span translate=no>_^_3_^_</span>  is the rank of the decomposition <span translate=no>_^_4_^_</span> </li>\n<li><span translate=no>_^_5_^_</span>  is the scaling factor <span translate=no>_^_6_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>  is the number of input features of the linear layer </li>\n<li><span translate=no>_^_1_^_</span>  is the number of output features of the linear layer </li>\n<li><span translate=no>_^_2_^_</span>  is a flag indicating if there is a bias parameter </li>\n<li><span translate=no>_^_3_^_</span>  is the rank of the decomposition <span translate=no>_^_4_^_</span> </li>\n<li><span translate=no>_^_5_^_</span>  is the scaling factor <span translate=no>_^_6_^_</span></li></ul>\n",
 "Annotated implementation of RoRA from paper LoRA: Low-Rank Adaptation of Large Language Models": "Annotated implementation of RoRA from paper LoRA: Low-Rank Adaptation of Large Language Models",
 "Low-Rank Adaptation (LoRA)": "Low-Rank Adaptation (LoRA)"
}