{
 "<h1><a href=\"https://nn.labml.ai/transformers/primer_ez/index.html\">Primer: Searching for Efficient Transformers for Language Modeling</a></h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper <a href=\"https://papers.labml.ai/paper/2109.08668\">Primer: Searching for Efficient Transformers for Language Modeling</a>.</p>\n<p>The authors do an evolutionary search for transformer architectures. They name the architecture found using the search as Primer (PRIMitives searched transformER). <strong>Primer EZ</strong> is the architecture with the two most robust modifications in Primer compared to  the original transformer. Primer EZ trains a lot faster than the vanilla transformer.</p>\n<h3>Squared ReLU</h3>\n<p>The most effective modification found by the search is using a square ReLU instead of ReLU in the <a href=\"https://nn.labml.ai/transformers/feed_forward.html\">position-wise feedforward module</a>.</p>\n<h3>Multi-DConv-Head Attention (MDHA)</h3>\n<p>The next effective modification is a depth-wise 3 X 1 convolution after multi-head projection  for queries, keys, and values. The convolution is along the sequence dimension and per channel (depth-wise). To be clear, if the number of channels in each head is d_k the convolution will have 1 X 3 kernels for each of the d_k channels.</p>\n<p><a href=\"https://nn.labml.ai/transformers/primer_ez/experiment.html\">Here is the experiment code</a>, for Primer EZ.</p>\n<p><a href=\"https://app.labml.ai/run/30adb7aa1ab211eca7310f80a114e8a4\"><span translate=no>_^_0_^_</span></a> </p>\n": "<h1><a href=\"https://nn.labml.ai/transformers/primer_ez/index.html\">\u5165\u95e8\uff1a\u4e3a\u8bed\u8a00\u5efa\u6a21\u5bfb\u627e\u9ad8\u6548\u7684\u53d8\u6362\u5668</a></h1>\n<p>\u8fd9\u662f\u8bba\u6587\u300a<a href=\"https://papers.labml.ai/paper/2109.08668\">\u5165\u95e8\uff1a\u4e3a\u8bed\u8a00\u5efa\u6a21\u5bfb\u627e\u9ad8\u6548\u7684\u53d8</a>\u6362\u5668\u300b\u7684 <a href=\"https://pytorch.org\">PyTorch</a> \u5b9e\u73b0\u3002</p>\n<p>\u4f5c\u8005\u5bf9\u53d8\u538b\u5668\u67b6\u6784\u8fdb\u884c\u4e86\u8fdb\u5316\u7814\u7a76\u3002\u4ed6\u4eec\u5c06\u4f7f\u7528\u641c\u7d22\u627e\u5230\u7684\u67b6\u6784\u547d\u540d\u4e3a Primer\uff08Primitives \u641c\u7d22\u4e86 Transformer\uff09\u3002\u4e0e\u539f\u59cb\u53d8\u538b\u5668\u76f8\u6bd4\uff0c<strong>Primer EZ</strong> \u662f\u5728 Primer \u4e2d\u8fdb\u884c\u4e86\u4e24\u6b21\u6700\u7a33\u5065\u4fee\u6539\u7684\u67b6\u6784\u3002Primer EZ \u7684\u8bad\u7ec3\u901f\u5ea6\u6bd4\u666e\u901a\u53d8\u538b\u5668\u5feb\u5f97\u591a\u3002</p>\n<h3>Squared Elu</h3>\n<p>\u641c\u7d22\u53d1\u73b0\u7684\u6700\u6709\u6548\u7684\u4fee\u6539\u662f\u5728<a href=\"https://nn.labml.ai/transformers/feed_forward.html\">\u4f4d\u7f6e\u524d\u9988\u6a21\u5757\u4e2d\u4f7f\u7528\u65b9\u5f62 RelU \u800c\u4e0d\u662f R</a> elU\u3002</p>\n<h3>\u591a dconv-Head \u6ce8\u610f\u529b (MDHA)</h3>\n<p>\u4e0b\u4e00\u4e2a\u6709\u6548\u7684\u4fee\u6539\u662f\u5728\u591a\u5934\u6295\u5f71\u4e4b\u540e\u5bf9\u67e5\u8be2\u3001\u952e\u548c\u503c\u8fdb\u884c\u6df1\u5ea6\u7684 3 X 1 \u5377\u79ef\u3002\u5377\u79ef\u6cbf\u5e8f\u5217\u7ef4\u5ea6\u548c\u6bcf\u4e2a\u901a\u9053\uff08\u6df1\u5ea6\u65b9\u9762\uff09\u8fdb\u884c\u3002\u9700\u8981\u660e\u786e\u7684\u662f\uff0c\u5982\u679c\u6bcf\u4e2a head \u4e2d\u7684\u901a\u9053\u6570\u4e3a d_k\uff0c\u5219\u5377\u79ef\u5c06\u4e3a\u6bcf\u4e2a d_k \u901a\u9053\u6709 1 X 3 \u4e2a\u5185\u6838\u3002</p>\n<p><a href=\"https://nn.labml.ai/transformers/primer_ez/experiment.html\">\u8fd9\u91cc\u662f Primer EZ \u7684\u5b9e\u9a8c\u4ee3\u7801</a>\u3002</p>\n<p><a href=\"https://app.labml.ai/run/30adb7aa1ab211eca7310f80a114e8a4\"><span translate=no>_^_0_^_</span></a></p>\n",
 "Primer: Searching for Efficient Transformers for Language Modeling": "\u5165\u95e8\uff1a\u4e3a\u8bed\u8a00\u5efa\u6a21\u5bfb\u627e\u9ad8\u6548\u7684\u53d8\u6362\u5668"
}